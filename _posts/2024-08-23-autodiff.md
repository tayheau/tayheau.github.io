---
title: "A quick review of Automatic Differentation and its modes"
date: 2024-08-23
layout: post
---
Automatic Differentiation (AutoDiff or AD) is a set of techniques used to evaluate the partial derivative of a function. Its strength comes from the fact that it's the most computationally effective way.

AD has a two-sided nature : it's partly symbolic and partly numerical since it keeps track of the expression (unlike the numerical method) and gives a final numerical value (unlike symbolic method).

## What AD is not 
### It's not numerical differentiation
Numerical Differentiation uses finite difference approximation.
For a multivariable scalar function \(f:\mathbb{R}^n->\mathbb{R}\), we can estimate the gradient \(\triangledown f = \left(\dfrac{\partial f}{\partial x_1}, ..., \dfrac{\partial f}{\partial x_n}\right)\) using 
$$\dfrac{\partial f(x)}{\partial x_i} \approx \frac{f(x+he_i) - f(x)}{h}$$
with \(e_i$ the $i^{th}\) unit vector and \(h\) very small and >0 (usually \(h \approx 10^{-5}\)).

It seems to be a good way to compute derivative since it's easy to implement, but it has the disadvantage of having a time complexity of \(\mathcal{O}(n)\) for a vector of dimension n ( it will be an issue for the case of a model with millions of parameters) and require to carefully select $h$.
Moreover, since it still an approximation, it can lead to huge numerical errors due to Truncation Error and Round-off Error.

![image](https://github.com/user-attachments/assets/a7f64f86-b0a0-4598-b26f-d6068121b327)


## It's not Symbolic Differentiation
