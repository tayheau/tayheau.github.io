---
title: "A quick review of Automatic Differentation and its modes"
date: 2024-08-23
layout: post
---
Automatic Differentiation (AutoDiff or AD) is a set of techniques used to evaluate the partial derivative of a function. Its strength comes from the fact that it's the most computationally effective way.

AD has a two-sided nature : it's partly symbolic and partly numerical since it keeps track of the expression (unlike the numerical method) and gives a final numerical value (unlike symbolic method).

## What AD is not 
### It's not numerical differentiation
Numerical Differentiation uses finite difference approximation.
For a multivariable scalar function \\(f:\mathbb{R}^n->\mathbb{R}\\), we can estimate the gradient \\(\triangledown f = \left(\dfrac{\partial f}{\partial x_1}, ..., \dfrac{\partial f}{\partial x_n}\right)\\) using 
<div style="display: flex; justify-content: center;">$$\dfrac{\partial f(x)}{\partial x_i} \approx \frac{f(x+he_i) - f(x)}{h}$$</div>
with \\(e_i\\) the \\(i^{th}\\) unit vector and \\(h\\) very small and >0 (usually \\(h \approx 10^{-5}\\)).

It seems to be a good way to compute derivative since it's easy to implement, but it has the disadvantage of having a time complexity of \\(\mathcal{O}(n)\\) for a vector of dimension n ( it will be an issue for the case of a model with millions of parameters) and require to carefully select \\(h\\).
Moreover, since it still an approximation, it can lead to huge numerical errors due to Truncation Error and Round-off Error.

<div style="text-align:center">
  <img src="https://github.com/user-attachments/assets/a7f64f86-b0a0-4598-b26f-d6068121b327" />
</div>


## It's not Symbolic Differentiation
Symbolic Differentitation is a straighforward application of the defined derivative expressions to the function. I.E. if \\(f(x) = g(x)h(x)\\) then 
<div style="display:flex; justify-content:center">$$\dfrac{\partial f}{\partial x} = \dfrac{\partial g(x)}{\partial x}h(x) + g(x)\dfrac{\partial h(x)}{\partial x}$$</div>

It can be very useful to get some insight into the evaluated function (such as finding \\(\dfrac{d}{dx}f(x) = 0\\)). The problem here is a phenomenon called *expression swell*, where calculation runtime is not efficient as they can get exponentially larger than the expression derivative they represent.

I.E. from this paper[[1]](#1):
Iterations of the logistic map \\(l_{n+1} = 4l_n(1-l_n)\text{, avec } l_1=x\\), illustrating the *expression swell*

| n   | \\(l_n\\)              | \\(\dfrac{d}{dx}l_n\\)                                       |
| --- | ------------------ | -------------------------------------------------------- |
| 1   | \\(x\\)                  | \\(1\\)                                                        |
| 2   | \\(4x(1-x)\\)          | \\(4(1-x)-4x\\)                                              |
| 3   | \\(16x(1-x)(1-2x)^2\\) | \\(16(1 − x)(1 − 2x)^2 − 16x(1 − 2x)^2 − 64x(1 − x)(1 − 2x)\\) |
| 4   | \\(64x(1−x)(1−2x)^2 (1 − 8x + 8x^2)^2\\) | \\(128x(1 − x)(−8 + 16x)(1 − 2x)^2(1 − 8x+8x^2)+64(1−x)(1−2x)^2(1−8x+ 8x^2)^2 −64x(1−2x)^2(1−8x+8x^2)^2 − 256x(1 − x)(1 − 2x)(1 − 8x + 8x^2)^2\\)  |



What could be interesting if we are just interested in the accurate numerical evaluation of the derivative and not their actual symbolic form would be to store only the value of intermediate sub-expressions in memory: 

*we could apply symbolic differentiation to all the element-wise operations and keep the intermediate numerical results* -> this is the AD in forward mode.

## AD and its two modes

The main idea of AD is to decompose the main expression into a composition of element-wise operations called *element trace*.

AD is mostly based on the fact that all numerical computation is a composition of basic operations which derivatives are known(Griewank and Walther, 2008), and that those derivatives can be combined through the Chain Rule.
These basic operations are usually composed of :
- Basic arithmetic operations
- Unary sign switch
- Transcendals functions such as \\(e^x\\), \\(ln(x)\\) and trigonometric functions.

For the rest of the text, we will take the following function to illustrate my words: 
<div style="display: flex; justify-content:center;">$$f(a, b) = b*sin(a) + b^2 = r$$</div>  and we will represent by \\(w_i, i=0, ..., l\\) the intermediate variables.  Let's compute the evaluation trace of \\(f\\), and represent it in a computing graph, which is, as you will notice, a Directed Acyclic Graph (DAG).

| intermediates vars | expressions | values   |
| ------------------ | ----------- | -------- |
| \\(a\\)               | \\(a\\)         | \\(2\\)      |
| \\(b\\)                | \\(b\\)         | \\(5\\)      |
| \\(w_0\\)              | \\(sin(a)\\)    | \\(0.909\\)  |
| \\(w_1\\)              | \\(b*w_0\\)     | \\(1.818\\)  |
| \\(w_2\\)              | \\(b^2\\)       | \\(25\\)     |
| \\(r\\)                | \\(w_1 + w_2\\) | \\(26.818\\) |

<pre class="mermaid" style="display:flex; justify-content:center">
flowchart LR
A[$$a$$] --> B(( $$w_0$$ ))
C[$$b$$] --> D(( $$w_1$$ ))
B --> D
C --> E(( $$w_2$$ ))
E --> F(( $$r$$ ))
D --> F
</pre>


<div style="display:flex; justify-content:center;">

$$
\renewcommand{\arraystretch}{1.5}
\begin{array}{|c|c|} 
\hline 
\text{Visited Value in the backward pass} & \text{Result of the attribued trace() function} \\ \hline
r & \begin{array}{c} \bar{w_1} \mathrel{+}= \bar{r}\\ \bar{w_2} \mathrel{+}= \bar{r} \end{array} \\ \hline 
w_1 & \begin{array}{c} \bar{b} \mathrel{+}= w_0 \cdot \bar{w_1} \\ \bar{w_0} \mathrel{+}= b \cdot \bar{w_1} \end{array} \\ \hline 
w_2 & \bar{b} \mathrel{+}= 2b \cdot \bar{w_2} \\ \hline 
w_0 & \bar{a} \mathrel{+}= \cos(a) \cdot \bar{w_0} \\ \hline 
b & \text{None} \\ \hline 
a & \text{None} \\ \hline 
\end{array}
$$

</div>

## References
<a id="1">[1]</a> : Baydin et al. __"Automatic differentiation in machine learning: a survey"__. The Journal of Machine Learning Research, 18(153):1--43, 2018. [link](https://arxiv.org/abs/1502.05767)
